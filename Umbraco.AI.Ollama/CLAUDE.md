# Umbraco.AI.Ollama

Ollama provider plugin for Umbraco.AI, enabling integration with locally-hosted or remote Ollama instances.

## Overview

This provider plugin allows Umbraco.AI to use Ollama for AI capabilities. Ollama lets you run large language models locally, providing privacy-focused and cost-effective AI without external API dependencies.

## Key Features

- Support for all Ollama chat models (Llama, Mistral, Gemma, CodeLlama, etc.)
- Local development with default localhost:11434 configuration
- Optional authentication for remote/managed instances
- Automatic model discovery
- Streaming and non-streaming chat completions

## Installation

```bash
dotnet add package Umbraco.AI.Ollama
```

## Configuration

See [README.md](README.md) for detailed setup and usage instructions.

## Links

- [Documentation](https://github.com/umbraco/Umbraco.AI/tree/main/Umbraco.AI.Ollama)
- [Ollama Website](https://ollama.ai)
- [Report Issues](https://github.com/umbraco/Umbraco.AI/issues)
